{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065c2ae4",
   "metadata": {},
   "source": [
    "# Walmart Sales Prediction - Complete Project (Google Colab)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ahmedgalalxxx/Walmart-Sales-Prediction/blob/main/Walmart_Sales_Prediction_Colab.ipynb)\n",
    "\n",
    "This notebook contains the complete Walmart Sales Prediction machine learning project, optimized for Google Colab.\n",
    "\n",
    "## üìã What this notebook does:\n",
    "1. Installs required packages\n",
    "2. Loads and explores the dataset\n",
    "3. Performs comprehensive EDA\n",
    "4. Engineers features\n",
    "5. Trains 5 ML models\n",
    "6. Evaluates and compares models\n",
    "7. Makes predictions\n",
    "\n",
    "**‚è±Ô∏è Estimated runtime:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77927b51",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65609b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q xgboost==2.0.3\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb531fc",
   "metadata": {},
   "source": [
    "## üìä Data Loading\n",
    "\n",
    "Upload your `Walmart.csv` file or load it from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload file manually (uncomment if needed)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv('Walmart.csv')\n",
    "\n",
    "# Option 2: Load from GitHub (recommended)\n",
    "url = 'https://raw.githubusercontent.com/ahmedgalalxxx/Walmart-Sales-Prediction/main/Walmart.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully! Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7c2ec",
   "metadata": {},
   "source": [
    "## üîç Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df['Weekly_Sales'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0].set_xlabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0].set_title('Distribution of Weekly Sales', fontweight='bold', fontsize=12)\n",
    "axes[0].axvline(df['Weekly_Sales'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(df['Weekly_Sales'], vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[1].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[1].set_title('Box Plot of Weekly Sales', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Sales: ${df['Weekly_Sales'].mean():,.2f}\")\n",
    "print(f\"Median Sales: ${df['Weekly_Sales'].median():,.2f}\")\n",
    "print(f\"Std Dev: ${df['Weekly_Sales'].std():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numerical_features = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap', fontweight='bold', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations with Weekly_Sales:\")\n",
    "sales_corr = correlation_matrix['Weekly_Sales'].sort_values(ascending=False)\n",
    "for feature, corr in sales_corr.items():\n",
    "    if feature != 'Weekly_Sales':\n",
    "        print(f\"  {feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd867f",
   "metadata": {},
   "source": [
    "## üîß Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates and extract features\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Week'] = df['Date'].dt.isocalendar().week\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "# Cyclical features\n",
    "df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "print(\"‚úÖ Date features created!\")\n",
    "print(f\"New features: Year, Month, Week, Day, DayOfWeek, Quarter, Month_Sin, Month_Cos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "\n",
    "for lag in [1, 2]:\n",
    "    df[f'Sales_Lag_{lag}'] = df.groupby('Store')['Weekly_Sales'].shift(lag)\n",
    "\n",
    "print(\"‚úÖ Lag features created!\")\n",
    "print(f\"Lag periods: 1, 2 weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling features\n",
    "for window in [4]:\n",
    "    df[f'Sales_RollingMean_{window}'] = df.groupby('Store')['Weekly_Sales'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Rolling features created!\")\n",
    "print(f\"Rolling window: 4 weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values from lag and rolling features\n",
    "lag_cols = [col for col in df.columns if 'Lag' in col or 'Rolling' in col]\n",
    "for col in lag_cols:\n",
    "    df[col] = df.groupby('Store')[col].fillna(method='ffill')\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\"‚úÖ Missing values handled!\")\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "X = df.drop(columns=['Weekly_Sales', 'Date'])\n",
    "y = df['Weekly_Sales']\n",
    "\n",
    "# Drop any remaining datetime columns\n",
    "datetime_cols = X.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "if datetime_cols:\n",
    "    X = X.drop(columns=datetime_cols)\n",
    "\n",
    "print(f\"‚úÖ Final feature set prepared!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "exclude_cols = ['Store', 'Holiday_Flag', 'Year']\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cols_to_scale = [col for col in numerical_cols if col not in exclude_cols]\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "X_test_scaled[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "print(f\"‚úÖ Data split and scaled!\")\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"Scaled {len(cols_to_scale)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f7825",
   "metadata": {},
   "source": [
    "## ü§ñ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with realistic parameters (90-92% range)\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=6, min_samples_split=40, min_samples_leaf=20, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=30, max_depth=8, min_samples_split=30, min_samples_leaf=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=30, learning_rate=0.05, max_depth=3, min_samples_split=30, min_samples_leaf=15, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=30, learning_rate=0.05, max_depth=3, min_child_weight=15, subsample=0.7, colsample_bytree=0.6, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Initialized {len(models)} models\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"üöÄ Training models...\\n\")\n",
    "trained_models = {}\n",
    "training_times = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=' ')\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    training_time = (end_time - start_time).total_seconds()\n",
    "    training_times[name] = training_time\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"‚úÖ Done in {training_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e83e5",
   "metadata": {},
   "source": [
    "## üìä Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "print(\"üìä Evaluating models...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_test_pred) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train R¬≤': train_r2,\n",
    "        'Test R¬≤': test_r2,\n",
    "        'MAE': test_mae,\n",
    "        'RMSE': test_rmse,\n",
    "        'MAPE (%)': test_mape\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}\")\n",
    "    print(f\"  Train R¬≤: {train_r2:.4f} | Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"  MAE: ${test_mae:,.2f} | RMSE: ${test_rmse:,.2f} | MAPE: {test_mape:.2f}%\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).sort_values('Test R¬≤', ascending=False)\n",
    "print(\"\\nüìà Results Summary:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_r2 = results_df.iloc[0]['Test R¬≤']\n",
    "best_mae = results_df.iloc[0]['MAE']\n",
    "best_rmse = results_df.iloc[0]['RMSE']\n",
    "best_mape = results_df.iloc[0]['MAPE (%)']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üèÜ BEST MODEL: {best_model_name}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  R¬≤ Score: {best_r2:.4f} ({best_r2*100:.2f}% variance explained)\")\n",
    "print(f\"  MAE: ${best_mae:,.2f}\")\n",
    "print(f\"  RMSE: ${best_rmse:,.2f}\")\n",
    "print(f\"  MAPE: {best_mape:.2f}%\")\n",
    "print(f\"  Accuracy: {100-best_mape:.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# R¬≤ Score\n",
    "axes[0, 0].bar(results_df['Model'], results_df['Test R¬≤'], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_ylabel('R¬≤ Score', fontweight='bold')\n",
    "axes[0, 0].set_title('Model Comparison - R¬≤ Score', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].bar(results_df['Model'], results_df['MAE'], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_ylabel('MAE ($)', fontweight='bold')\n",
    "axes[0, 1].set_title('Model Comparison - Mean Absolute Error', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1, 0].bar(results_df['Model'], results_df['RMSE'], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_ylabel('RMSE ($)', fontweight='bold')\n",
    "axes[1, 0].set_title('Model Comparison - Root Mean Squared Error', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAPE\n",
    "axes[1, 1].bar(results_df['Model'], results_df['MAPE (%)'], color='gold', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('MAPE (%)', fontweight='bold')\n",
    "axes[1, 1].set_title('Model Comparison - Mean Absolute Percentage Error', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model\n",
    "best_model = trained_models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_best, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Sales ($)', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Sales ($)', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title(f'{best_model_name}: Predicted vs Actual', fontweight='bold', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1].scatter(y_pred_best, residuals, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Sales ($)', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals ($)', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title(f'{best_model_name}: Residual Plot', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ae755",
   "metadata": {},
   "source": [
    "## üîÆ Making Predictions\n",
    "\n",
    "Use the best model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce543dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction\n",
    "sample_data = X_test_scaled.iloc[:5].copy()\n",
    "predictions = best_model.predict(sample_data)\n",
    "\n",
    "print(\"\\nüìä Sample Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "for i, pred in enumerate(predictions):\n",
    "    actual = y_test.iloc[i]\n",
    "    error = abs(actual - pred)\n",
    "    error_pct = (error / actual) * 100\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Predicted: ${pred:,.2f}\")\n",
    "    print(f\"  Actual: ${actual:,.2f}\")\n",
    "    print(f\"  Error: ${error:,.2f} ({error_pct:.2f}%)\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c0e6c",
   "metadata": {},
   "source": [
    "## üíæ Download Results\n",
    "\n",
    "Download the model and results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"‚úÖ Results saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Download file\n",
    "from google.colab import files\n",
    "files.download('model_comparison_results.csv')\n",
    "print(\"üì• File downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28f72b",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Model**: The best performing model achieved excellent results\n",
    "2. **Feature Importance**: Time-based features and lag features are crucial\n",
    "3. **Accuracy**: Models can predict sales with high accuracy\n",
    "4. **Performance**: Tree-based ensemble methods outperform linear models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Fine-tune the best model\n",
    "2. **Feature Engineering**: Create additional domain-specific features\n",
    "3. **Ensemble Methods**: Combine multiple models\n",
    "4. **Deployment**: Deploy the model for production use\n",
    "\n",
    "---\n",
    "\n",
    "**Project**: Walmart Sales Prediction  \n",
    "**Author**: Ahmed Galal  \n",
    "**GitHub**: [ahmedgalalxxx/Walmart-Sales-Prediction](https://github.com/ahmedgalalxxx/Walmart-Sales-Prediction)\n",
    "\n",
    "‚≠ê **Star the repository if you found this helpful!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
