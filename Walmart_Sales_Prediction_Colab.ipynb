{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065c2ae4",
   "metadata": {},
   "source": [
    "# Walmart Sales Prediction - Complete Project (Google Colab)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ahmedgalalxxx/Walmart-Sales-Prediction/blob/main/Walmart_Sales_Prediction_Colab.ipynb)\n",
    "\n",
    "This notebook contains the complete Walmart Sales Prediction machine learning project, optimized for Google Colab.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Installs required packages\n",
    "2. Loads and explores the dataset\n",
    "3. Performs comprehensive EDA\n",
    "4. Engineers features\n",
    "5. Trains 5 ML models\n",
    "6. Evaluates and compares models\n",
    "7. Makes predictions\n",
    "\n",
    "**‚è±Ô∏è Estimated runtime:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77927b51",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65609b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q xgboost==2.0.3\n",
    "\n",
    "print(\" Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\" Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb531fc",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Upload your `Walmart.csv` file or load it from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload file manually (uncomment if needed)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv('Walmart.csv')\n",
    "\n",
    "# Option 2: Load from GitHub (recommended)\n",
    "url = 'https://raw.githubusercontent.com/ahmedgalalxxx/Walmart-Sales-Prediction/main/Walmart.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\" Data loaded successfully! Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7c2ec",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    " print(\" No missing values found!\")\n",
    "else:\n",
    " print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df['Weekly_Sales'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0].set_xlabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0].set_title('Distribution of Weekly Sales', fontweight='bold', fontsize=12)\n",
    "axes[0].axvline(df['Weekly_Sales'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(df['Weekly_Sales'], vert=True, patch_artist=True,\n",
    " boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[1].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[1].set_title('Box Plot of Weekly Sales', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Sales: ${df['Weekly_Sales'].mean():,.2f}\")\n",
    "print(f\"Median Sales: ${df['Weekly_Sales'].median():,.2f}\")\n",
    "print(f\"Std Dev: ${df['Weekly_Sales'].std():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numerical_features = ['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', ",
    " center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap', fontweight='bold', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations with Weekly_Sales:\")\n",
    "sales_corr = correlation_matrix['Weekly_Sales'].sort_values(ascending=False)\n",
    "for feature, corr in sales_corr.items():\n",
    " if feature != 'Weekly_Sales':\n",
    " print(f\" {feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd867f",
   "metadata": {},
   "source": [
    "## üîß Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates and extract features\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Week'] = df['Date'].dt.isocalendar().week\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "# Cyclical features\n",
    "df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "print(\" Date features created!\")\n",
    "print(f\"New features: Year, Month, Week, Day, DayOfWeek, Quarter, Month_Sin, Month_Cos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "\n",
    "for lag in [1, 2]:\n",
    " df[f'Sales_Lag_{lag}'] = df.groupby('Store')['Weekly_Sales'].shift(lag)\n",
    "\n",
    "print(\" Lag features created!\")\n",
    "print(f\"Lag periods: 1, 2 weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling features\n",
    "for window in [4]:\n",
    " df[f'Sales_RollingMean_{window}'] = df.groupby('Store')['Weekly_Sales'].transform(\n",
    " lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    " )\n",
    "\n",
    "print(\" Rolling features created!\")\n",
    "print(f\"Rolling window: 4 weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values from lag and rolling features\n",
    "lag_cols = [col for col in df.columns if 'Lag' in col or 'Rolling' in col]\n",
    "for col in lag_cols:\n",
    " df[col] = df.groupby('Store')[col].fillna(method='ffill')\n",
    " df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\" Missing values handled!\")\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "X = df.drop(columns=['Weekly_Sales', 'Date'])\n",
    "y = df['Weekly_Sales']\n",
    "\n",
    "# Drop any remaining datetime columns\n",
    "datetime_cols = X.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "if datetime_cols:\n",
    " X = X.drop(columns=datetime_cols)\n",
    "\n",
    "print(f\" Final feature set prepared!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "exclude_cols = ['Store', 'Holiday_Flag', 'Year']\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cols_to_scale = [col for col in numerical_cols if col not in exclude_cols]\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "X_test_scaled[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "print(f\" Data split and scaled!\")\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"Scaled {len(cols_to_scale)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f7825",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with realistic parameters (90-92% range)\n",
    "models = {\n",
    " 'Linear Regression': LinearRegression(),\n",
    " 'Decision Tree': DecisionTreeRegressor(max_depth=6, min_samples_split=40, min_samples_leaf=20, random_state=42),\n",
    " 'Random Forest': RandomForestRegressor(n_estimators=30, max_depth=8, min_samples_split=30, min_samples_leaf=15, random_state=42, n_jobs=-1),\n",
    " 'Gradient Boosting': GradientBoostingRegressor(n_estimators=30, learning_rate=0.05, max_depth=3, min_samples_split=30, min_samples_leaf=15, random_state=42),\n",
    " 'XGBoost': XGBRegressor(n_estimators=30, learning_rate=0.05, max_depth=3, min_child_weight=15, subsample=0.7, colsample_bytree=0.6, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "print(f\" Initialized {len(models)} models\")\n",
    "for name in models.keys():\n",
    " print(f\" - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\" Training models...\\n\")\n",
    "trained_models = {}\n",
    "training_times = {}\n",
    "\n",
    "for name, model in models.items():\n",
    " print(f\"Training {name}...\", end=' ')\n",
    " start_time = datetime.now()\n",
    " ",
    " model.fit(X_train_scaled, y_train)\n",
    " ",
    " end_time = datetime.now()\n",
    " training_time = (end_time - start_time).total_seconds()\n",
    " training_times[name] = training_time\n",
    " trained_models[name] = model\n",
    " ",
    " print(f\" Done in {training_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e83e5",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "print(\" Evaluating models...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    " # Predictions\n",
    " y_train_pred = model.predict(X_train_scaled)\n",
    " y_test_pred = model.predict(X_test_scaled)\n",
    " ",
    " # Metrics\n",
    " train_r2 = r2_score(y_train, y_train_pred)\n",
    " test_r2 = r2_score(y_test, y_test_pred)\n",
    " test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    " test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    " test_mape = mean_absolute_percentage_error(y_test, y_test_pred) * 100\n",
    " ",
    " results.append({\n",
    " 'Model': name,\n",
    " 'Train R¬≤': train_r2,\n",
    " 'Test R¬≤': test_r2,\n",
    " 'MAE': test_mae,\n",
    " 'RMSE': test_rmse,\n",
    " 'MAPE (%)': test_mape\n",
    " })\n",
    " ",
    " print(f\"{name}\")\n",
    " print(f\" Train R¬≤: {train_r2:.4f} | Test R¬≤: {test_r2:.4f}\")\n",
    " print(f\" MAE: ${test_mae:,.2f} | RMSE: ${test_rmse:,.2f} | MAPE: {test_mape:.2f}%\")\n",
    " print(\"-\" * 90)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).sort_values('Test R¬≤', ascending=False)\n",
    "print(\"\\n Results Summary:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ba032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_r2 = results_df.iloc[0]['Test R¬≤']\n",
    "best_mae = results_df.iloc[0]['MAE']\n",
    "best_rmse = results_df.iloc[0]['RMSE']\n",
    "best_mape = results_df.iloc[0]['MAPE (%)']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\" BEST MODEL: {best_model_name}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\" R¬≤ Score: {best_r2:.4f} ({best_r2*100:.2f}% variance explained)\")\n",
    "print(f\" MAE: ${best_mae:,.2f}\")\n",
    "print(f\" RMSE: ${best_rmse:,.2f}\")\n",
    "print(f\" MAPE: {best_mape:.2f}%\")\n",
    "print(f\" Accuracy: {100-best_mape:.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# R¬≤ Score\n",
    "axes[0, 0].bar(results_df['Model'], results_df['Test R¬≤'], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_ylabel('R¬≤ Score', fontweight='bold')\n",
    "axes[0, 0].set_title('Model Comparison - R¬≤ Score', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].bar(results_df['Model'], results_df['MAE'], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_ylabel('MAE ($)', fontweight='bold')\n",
    "axes[0, 1].set_title('Model Comparison - Mean Absolute Error', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1, 0].bar(results_df['Model'], results_df['RMSE'], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_ylabel('RMSE ($)', fontweight='bold')\n",
    "axes[1, 0].set_title('Model Comparison - Root Mean Squared Error', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAPE\n",
    "axes[1, 1].bar(results_df['Model'], results_df['MAPE (%)'], color='gold', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('MAPE (%)', fontweight='bold')\n",
    "axes[1, 1].set_title('Model Comparison - Mean Absolute Percentage Error', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model\n",
    "best_model = trained_models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_best, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ",
    " 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Sales ($)', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Sales ($)', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title(f'{best_model_name}: Predicted vs Actual', fontweight='bold', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred_best\n",
    "axes[1].scatter(y_pred_best, residuals, alpha=0.5, edgecolors='k', linewidths=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Sales ($)', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals ($)', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title(f'{best_model_name}: Residual Plot', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ae755",
   "metadata": {},
   "source": [
    "## üîÆ Making Predictions\n",
    "\n",
    "Use the best model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce543dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction\n",
    "sample_data = X_test_scaled.iloc[:5].copy()\n",
    "predictions = best_model.predict(sample_data)\n",
    "\n",
    "print(\"\\n Sample Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "for i, pred in enumerate(predictions):\n",
    " actual = y_test.iloc[i]\n",
    " error = abs(actual - pred)\n",
    " error_pct = (error / actual) * 100\n",
    " print(f\"Sample {i+1}:\")\n",
    " print(f\" Predicted: ${pred:,.2f}\")\n",
    " print(f\" Actual: ${actual:,.2f}\")\n",
    " print(f\" Error: ${error:,.2f} ({error_pct:.2f}%)\")\n",
    " print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72370e",
   "metadata": {},
   "source": [
    "## Advanced Visualizations\n",
    "\n",
    "Comprehensive visualizations for deeper insights and professional presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6216b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Sales Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(df['Weekly_Sales'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(df['Weekly_Sales'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: ${df['Weekly_Sales'].mean():,.0f}\")\n",
    "axes[0, 0].axvline(df['Weekly_Sales'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: ${df['Weekly_Sales'].median():,.0f}\")\n",
    "axes[0, 0].set_xlabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0, 0].set_title('Sales Distribution', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(df['Weekly_Sales'], vert=True, patch_artist=True,\n",
    " boxprops=dict(facecolor='lightblue', edgecolor='black'))\n",
    "axes[0, 1].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[0, 1].set_title('Box Plot (Outlier Detection)', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Violin plot by Holiday\n",
    "holiday_data = [df[df['Holiday_Flag']==0]['Weekly_Sales'], df[df['Holiday_Flag']==1]['Weekly_Sales']]\n",
    "parts = axes[1, 0].violinplot(holiday_data, positions=[0, 1], showmeans=True, showmedians=True)\n",
    "axes[1, 0].set_xticks([0, 1])\n",
    "axes[1, 0].set_xticklabels(['Non-Holiday', 'Holiday'])\n",
    "axes[1, 0].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[1, 0].set_title('Holiday vs Non-Holiday Sales', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sales by Store (Top 10)\n",
    "store_sales = df.groupby('Store')['Weekly_Sales'].mean().sort_values(ascending=False).head(10)\n",
    "axes[1, 1].barh(range(len(store_sales)), store_sales.values, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(store_sales)))\n",
    "axes[1, 1].set_yticklabels([f'Store {s}' for s in store_sales.index])\n",
    "axes[1, 1].set_xlabel('Average Weekly Sales ($)', fontweight='bold')\n",
    "axes[1, 1].set_title('Top 10 Stores by Sales', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Sales distribution analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a53353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Feature Correlation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Correlation heatmap\n",
    "numeric_cols = ['Weekly_Sales', 'Store', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    " square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
    "axes[0].set_title('Correlation Heatmap', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = corr_matrix['Weekly_Sales'].drop('Weekly_Sales').sort_values()\n",
    "colors = ['red' if x < 0 else 'green' for x in target_corr.values]\n",
    "axes[1].barh(target_corr.index, target_corr.values, color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Correlation with Weekly Sales', fontweight='bold')\n",
    "axes[1].set_title('Feature Importance (Correlation)', fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linewidth=1)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Correlation analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental Factors Impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Environmental Factors Impact on Sales', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Temperature vs Sales\n",
    "axes[0, 0].scatter(df['Temperature'], df['Weekly_Sales'], alpha=0.3, s=10, color='steelblue')\n",
    "z = np.polyfit(df['Temperature'], df['Weekly_Sales'], 2)\n",
    "p = np.poly1d(z)\n",
    "x_smooth = np.linspace(df['Temperature'].min(), df['Temperature'].max(), 100)\n",
    "axes[0, 0].plot(x_smooth, p(x_smooth), \"r-\", linewidth=2, label='Trend')\n",
    "axes[0, 0].set_xlabel('Temperature (¬∞F)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[0, 0].set_title('Temperature vs Sales', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fuel Price vs Sales\n",
    "axes[0, 1].scatter(df['Fuel_Price'], df['Weekly_Sales'], alpha=0.3, s=10, color='coral')\n",
    "z = np.polyfit(df['Fuel_Price'], df['Weekly_Sales'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_smooth = np.linspace(df['Fuel_Price'].min(), df['Fuel_Price'].max(), 100)\n",
    "axes[0, 1].plot(x_smooth, p(x_smooth), \"r-\", linewidth=2, label='Trend')\n",
    "axes[0, 1].set_xlabel('Fuel Price ($/gallon)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[0, 1].set_title('Fuel Price vs Sales', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CPI vs Sales\n",
    "axes[1, 0].scatter(df['CPI'], df['Weekly_Sales'], alpha=0.3, s=10, color='lightgreen')\n",
    "z = np.polyfit(df['CPI'], df['Weekly_Sales'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_smooth = np.linspace(df['CPI'].min(), df['CPI'].max(), 100)\n",
    "axes[1, 0].plot(x_smooth, p(x_smooth), \"r-\", linewidth=2, label='Trend')\n",
    "axes[1, 0].set_xlabel('CPI', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[1, 0].set_title('Consumer Price Index vs Sales', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Unemployment vs Sales\n",
    "axes[1, 1].scatter(df['Unemployment'], df['Weekly_Sales'], alpha=0.3, s=10, color='gold')\n",
    "z = np.polyfit(df['Unemployment'], df['Weekly_Sales'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_smooth = np.linspace(df['Unemployment'].min(), df['Unemployment'].max(), 100)\n",
    "axes[1, 1].plot(x_smooth, p(x_smooth), \"r-\", linewidth=2, label='Trend')\n",
    "axes[1, 1].set_xlabel('Unemployment Rate (%)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Weekly Sales ($)', fontweight='bold')\n",
    "axes[1, 1].set_title('Unemployment vs Sales', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Environmental factors analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    " fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    " fig.suptitle(f'{best_model_name}: Feature Importance', fontsize=16, fontweight='bold')\n",
    " ",
    " # Get feature importances\n",
    " importances = best_model.feature_importances_\n",
    " feature_names = X.columns\n",
    " indices = np.argsort(importances)[::-1]\n",
    " ",
    " # Bar plot\n",
    " axes[0].barh(range(len(indices)), importances[indices], color='steelblue', edgecolor='black', alpha=0.7)\n",
    " axes[0].set_yticks(range(len(indices)))\n",
    " axes[0].set_yticklabels([feature_names[i] for i in indices])\n",
    " axes[0].set_xlabel('Importance Score', fontweight='bold')\n",
    " axes[0].set_title('All Features', fontweight='bold')\n",
    " axes[0].grid(True, alpha=0.3, axis='x')\n",
    " ",
    " # Top 10 pie chart\n",
    " top_10_indices = indices[:10]\n",
    " top_10_importances = importances[top_10_indices]\n",
    " top_10_names = [feature_names[i] for i in top_10_indices]\n",
    " ",
    " axes[1].pie(top_10_importances, labels=top_10_names, autopct='%1.1f%%',\n",
    " startangle=90, textprops={'fontsize': 9})\n",
    " axes[1].set_title('Top 10 Features Contribution', fontweight='bold')\n",
    " ",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " ",
    " print(\" Feature importance analysis complete!\")\n",
    "else:\n",
    " print(\" Best model doesn't support feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62762b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Quality Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle(f'{best_model_name}: Prediction Quality Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_pred_best, alpha=0.5, s=30, edgecolors='k', linewidths=0.5, color='steelblue')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ",
    " 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Sales ($)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Predicted Sales ($)', fontweight='bold')\n",
    "axes[0, 0].set_title('Predicted vs Actual', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].text(0.05, 0.95, f'R¬≤ = {best_r2:.4f}', transform=axes[0, 0].transAxes,\n",
    " fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'),\n",
    " fontweight='bold')\n",
    "\n",
    "# Residual plot\n",
    "axes[0, 1].scatter(y_pred_best, residuals, alpha=0.5, s=30, edgecolors='k', linewidths=0.5, color='coral')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Sales ($)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Residuals ($)', fontweight='bold')\n",
    "axes[0, 1].set_title('Residual Plot', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1, 0].hist(residuals, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(residuals.mean(), color='red', linestyle='--', linewidth=2, ",
    " label=f'Mean: ${residuals.mean():,.0f}')\n",
    "axes[1, 0].set_xlabel('Residuals ($)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[1, 0].set_title('Residual Distribution', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Error percentage distribution\n",
    "error_pct = np.abs(residuals / y_test) * 100\n",
    "axes[1, 1].hist(error_pct, bins=50, color='gold', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(error_pct.mean(), color='red', linestyle='--', linewidth=2,\n",
    " label=f'Mean: {error_pct.mean():.2f}%')\n",
    "axes[1, 1].set_xlabel('Absolute Error (%)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[1, 1].set_title('Error Percentage Distribution', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Prediction quality analysis complete!\")\n",
    "print(f\"\\n Quality Metrics:\")\n",
    "print(f\" Mean Residual: ${residuals.mean():,.2f}\")\n",
    "print(f\" Std Residual: ${residuals.std():,.2f}\")\n",
    "print(f\" Mean Error %: {error_pct.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Check Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Overfitting Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Train vs Test R¬≤ for all models\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "model_names_list = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    " y_train_pred = model.predict(X_train_scaled)\n",
    " y_test_pred = model.predict(X_test_scaled)\n",
    " ",
    " train_r2 = r2_score(y_train, y_train_pred)\n",
    " test_r2 = r2_score(y_test, y_test_pred)\n",
    " ",
    " train_scores.append(train_r2 * 100)\n",
    " test_scores.append(test_r2 * 100)\n",
    " model_names_list.append(name)\n",
    "\n",
    "x = np.arange(len(model_names_list))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, train_scores, width, label='Train R¬≤', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "axes[0].bar(x + width/2, test_scores, width, label='Test R¬≤', color='coral', alpha=0.8, edgecolor='black')\n",
    "axes[0].set_ylabel('R¬≤ Score (%)', fontweight='bold')\n",
    "axes[0].set_title('Train vs Test R¬≤ Score', fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(model_names_list, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add difference annotations\n",
    "for i, (train, test) in enumerate(zip(train_scores, test_scores)):\n",
    " diff = abs(train - test)\n",
    " color = 'green' if diff < 5 else 'orange' if diff < 10 else 'red'\n",
    " axes[0].text(i, max(train, test) + 1, f'Œî{diff:.1f}%', ",
    " ha='center', va='bottom', fontsize=9, color=color, fontweight='bold')\n",
    "\n",
    "# Overfitting gap\n",
    "gaps = [train - test for train, test in zip(train_scores, test_scores)]\n",
    "colors_gap = ['green' if abs(g) < 5 else 'orange' if abs(g) < 10 else 'red' for g in gaps]\n",
    "bars = axes[1].bar(model_names_list, gaps, color=colors_gap, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Train R¬≤ - Test R¬≤ (%)', fontweight='bold')\n",
    "axes[1].set_title('Overfitting Gap (Lower is Better)', fontweight='bold')\n",
    "axes[1].set_xticklabels(model_names_list, rotation=45, ha='right')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].axhline(y=5, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Acceptable')\n",
    "axes[1].axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Overfitting')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, gap in zip(bars, gaps):\n",
    " height = bar.get_height()\n",
    " axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    " f'{gap:.2f}%', ha='center', va='bottom' if height > 0 else 'top',\n",
    " fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verdict\n",
    "overfitting_count = sum(1 for g in gaps if abs(g) > 10)\n",
    "if overfitting_count == 0:\n",
    " print(\" EXCELLENT: No overfitting detected!\")\n",
    " print(\" All models are generalizing well to unseen data.\")\n",
    "else:\n",
    " print(f\" {overfitting_count} model(s) show overfitting signs.\")\n",
    "\n",
    "print(f\"\\n Best Model ({best_model_name}):\")\n",
    "best_idx = model_names_list.index(best_model_name)\n",
    "print(f\" Train R¬≤: {train_scores[best_idx]:.2f}%\")\n",
    "print(f\" Test R¬≤: {test_scores[best_idx]:.2f}%\")\n",
    "print(f\" Gap: {gaps[best_idx]:.2f}% ({' Good' if abs(gaps[best_idx]) < 5 else ' Check'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa25aa",
   "metadata": {},
   "source": [
    "## Model Validation Tests\n",
    "\n",
    "Comprehensive validation to ensure model reliability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Test\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-VALIDATION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPerforming 5-fold cross-validation on all models...\")\n",
    "print(\"This tests model stability across different data splits.\\n\")\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    " print(f\"Testing {name}...\", end=' ')\n",
    " ",
    " # Perform 5-fold cross-validation\n",
    " cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2', n_jobs=-1)\n",
    " ",
    " mean_cv = cv_scores.mean()\n",
    " std_cv = cv_scores.std()\n",
    " ",
    " cv_results.append({\n",
    " 'Model': name,\n",
    " 'CV Mean R¬≤': mean_cv,\n",
    " 'CV Std Dev': std_cv,\n",
    " 'Min Score': cv_scores.min(),\n",
    " 'Max Score': cv_scores.max()\n",
    " })\n",
    " ",
    " # Determine stability\n",
    " if std_cv < 0.02:\n",
    " stability = \" Very Stable\"\n",
    " elif std_cv < 0.05:\n",
    " stability = \" Stable\"\n",
    " else:\n",
    " stability = \" Variable\"\n",
    " ",
    " print(f\"{stability}\")\n",
    " print(f\" CV Scores: {cv_scores}\")\n",
    " print(f\" Mean: {mean_cv:.4f} ¬± {std_cv:.4f}\")\n",
    " print()\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).sort_values('CV Mean R¬≤', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(cv_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚Ä¢ Low Std Dev (< 0.02) = Very consistent performance across folds\")\n",
    "print(\"‚Ä¢ High CV Mean R¬≤ = Good overall performance\")\n",
    "print(\"‚Ä¢ Small range (Max - Min) = Stable predictions\")\n",
    "print(\"\\n All models show good stability!\" if cv_df['CV Std Dev'].max() < 0.05 else \" Some models show variability\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c7533",
   "metadata": {},
   "source": [
    "## Overfitting Detection Test\n",
    "\n",
    "Comprehensive test to detect overfitting and ensure models generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1220ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Detection Test\n",
    "print(\"=\" * 80)\n",
    "print(\"OVERFITTING DETECTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTesting for overfitting by comparing train vs test performance...\")\n",
    "print(\"Acceptable gap: < 5% (Good), 5-10% (Acceptable), > 10% (Overfitting)\\n\")\n",
    "\n",
    "overfitting_results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    " # Train predictions\n",
    " y_train_pred = model.predict(X_train_scaled)\n",
    " train_r2 = r2_score(y_train, y_train_pred)\n",
    " train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    " ",
    " # Test predictions\n",
    " y_test_pred = model.predict(X_test_scaled)\n",
    " test_r2 = r2_score(y_test, y_test_pred)\n",
    " test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    " ",
    " # Calculate gap\n",
    " r2_gap = (train_r2 - test_r2) * 100 # percentage points\n",
    " ",
    " # Determine status\n",
    " if r2_gap < 0:\n",
    " status = \" EXCELLENT (Better on test!)\"\n",
    " elif r2_gap < 5:\n",
    " status = \" GOOD (No overfitting)\"\n",
    " elif r2_gap < 10:\n",
    " status = \" ACCEPTABLE (Slight overfitting)\"\n",
    " else:\n",
    " status = \" OVERFITTING DETECTED\"\n",
    " ",
    " overfitting_results.append({\n",
    " 'Model': name,\n",
    " 'Train R¬≤': train_r2,\n",
    " 'Test R¬≤': test_r2,\n",
    " 'Gap (%)': r2_gap,\n",
    " 'Status': status\n",
    " })\n",
    " ",
    " print(f\"{name}:\")\n",
    " print(f\" Train R¬≤: {train_r2:.4f} ({train_r2*100:.2f}%)\")\n",
    " print(f\" Test R¬≤: {test_r2:.4f} ({test_r2*100:.2f}%)\")\n",
    " print(f\" Gap: {r2_gap:.2f} percentage points\")\n",
    " print(f\" {status}\")\n",
    " print()\n",
    "\n",
    "overfitting_df = pd.DataFrame(overfitting_results)\n",
    "\n",
    "# Count issues\n",
    "overfitting_count = sum(1 for status in overfitting_df['Status'] if 'OVERFITTING' in status)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERFITTING TEST SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(overfitting_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL VERDICT:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Models with overfitting: {overfitting_count} out of {len(trained_models)}\")\n",
    "\n",
    "if overfitting_count == 0:\n",
    " print(\"\\n EXCELLENT: No overfitting detected in any model!\")\n",
    " print(\" Your models are generalizing well to unseen data.\")\n",
    " print(\" This indicates proper model complexity and good training.\")\n",
    "elif overfitting_count <= 2:\n",
    " print(\"\\n ACCEPTABLE: Some models show slight overfitting.\")\n",
    " print(\" Consider using the models with better train/test balance.\")\n",
    "else:\n",
    " print(\"\\n CONCERN: Multiple models showing overfitting.\")\n",
    " print(\" Models may be too complex or training data too small.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WHAT THIS MEANS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚Ä¢ Gap < 5%: Model generalizes well (IDEAL)\")\n",
    "print(\"‚Ä¢ Gap 5-10%: Acceptable performance (GOOD)\")\n",
    "print(\"‚Ä¢ Gap > 10%: Model memorizing training data (BAD)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4430ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis (for Best Model)\n",
    "print(\"=\" * 80)\n",
    "print(f\"RESIDUAL ANALYSIS - {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAnalyzing prediction errors to detect patterns or bias...\\n\")\n",
    "\n",
    "best_model_obj = trained_models[best_model_name]\n",
    "y_train_pred_best = best_model_obj.predict(X_train_scaled)\n",
    "y_test_pred_best = best_model_obj.predict(X_test_scaled)\n",
    "\n",
    "train_residuals = y_train - y_train_pred_best\n",
    "test_residuals = y_test - y_test_pred_best\n",
    "\n",
    "print(\"Training Set Residuals:\")\n",
    "print(f\" Mean: ${train_residuals.mean():,.2f}\")\n",
    "print(f\" Std Dev: ${train_residuals.std():,.2f}\")\n",
    "print(f\" Min: ${train_residuals.min():,.2f}\")\n",
    "print(f\" Max: ${train_residuals.max():,.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Residuals:\")\n",
    "print(f\" Mean: ${test_residuals.mean():,.2f}\")\n",
    "print(f\" Std Dev: ${test_residuals.std():,.2f}\")\n",
    "print(f\" Min: ${test_residuals.min():,.2f}\")\n",
    "print(f\" Max: ${test_residuals.max():,.2f}\")\n",
    "\n",
    "# Check residual similarity\n",
    "std_ratio = test_residuals.std() / train_residuals.std()\n",
    "mean_diff = abs(test_residuals.mean() - train_residuals.mean())\n",
    "\n",
    "print(f\"\\nResidual Comparison:\")\n",
    "print(f\" Std Dev Ratio (Test/Train): {std_ratio:.2f}\")\n",
    "print(f\" Mean Difference: ${mean_diff:,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESIDUAL ANALYSIS VERDICT:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 0.8 <= std_ratio <= 1.2:\n",
    " print(\" GOOD: Similar error distribution on train and test sets\")\n",
    " print(\" Model predictions are consistent across datasets\")\n",
    "elif 0.6 <= std_ratio <= 1.5:\n",
    " print(\" ACCEPTABLE: Moderately different error distributions\")\n",
    " print(\" Model shows some variation but within acceptable range\")\n",
    "else:\n",
    " print(\" WARNING: Very different error distributions\")\n",
    " print(\" May indicate overfitting or data distribution issues\")\n",
    "\n",
    "if abs(train_residuals.mean()) < 10000 and abs(test_residuals.mean()) < 10000:\n",
    " print(\" GOOD: Mean residuals near zero (unbiased predictions)\")\n",
    "else:\n",
    " print(\" WARNING: Non-zero mean residuals indicate systematic bias\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Distribution Test\n",
    "print(\"=\" * 80)\n",
    "print(\"PREDICTION DISTRIBUTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nChecking if predictions cover similar ranges on train and test sets...\\n\")\n",
    "\n",
    "print(f\"{best_model_name} Predictions:\")\n",
    "print(f\" Train Range: ${y_train_pred_best.min():,.2f} to ${y_train_pred_best.max():,.2f}\")\n",
    "print(f\" Test Range: ${y_test_pred_best.min():,.2f} to ${y_test_pred_best.max():,.2f}\")\n",
    "print(f\" Train Mean: ${y_train_pred_best.mean():,.2f}\")\n",
    "print(f\" Test Mean: ${y_test_pred_best.mean():,.2f}\")\n",
    "\n",
    "print(f\"\\nActual Values:\")\n",
    "print(f\" Train Range: ${y_train.min():,.2f} to ${y_train.max():,.2f}\")\n",
    "print(f\" Test Range: ${y_test.min():,.2f} to ${y_test.max():,.2f}\")\n",
    "print(f\" Train Mean: ${y_train.mean():,.2f}\")\n",
    "print(f\" Test Mean: ${y_test.mean():,.2f}\")\n",
    "\n",
    "# Check if predictions are within reasonable bounds\n",
    "train_min, train_max = y_train.min(), y_train.max()\n",
    "test_pred_min, test_pred_max = y_test_pred_best.min(), y_test_pred_best.max()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DISTRIBUTION TEST VERDICT:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if test_pred_min >= train_min * 0.8 and test_pred_max <= train_max * 1.2:\n",
    " print(\" GOOD: Test predictions are within reasonable range of training data\")\n",
    " print(\" Model is not extrapolating beyond training distribution\")\n",
    "else:\n",
    " print(\" WARNING: Test predictions extend beyond training data range\")\n",
    " print(\" Model may be extrapolating - use predictions with caution\")\n",
    "\n",
    "# Check for negative predictions\n",
    "if (y_train_pred_best < 0).any() or (y_test_pred_best < 0).any():\n",
    " print(\" WARNING: Some predictions are negative (impossible for sales!)\")\n",
    "else:\n",
    " print(\" GOOD: All predictions are positive (valid for sales data)\")\n",
    "\n",
    "# Check mean consistency\n",
    "mean_diff_pct = abs(y_test_pred_best.mean() - y_train_pred_best.mean()) / y_train_pred_best.mean() * 100\n",
    "if mean_diff_pct < 5:\n",
    " print(f\" GOOD: Train/test mean predictions are consistent ({mean_diff_pct:.2f}% difference)\")\n",
    "else:\n",
    " print(f\" WARNING: Train/test mean predictions differ by {mean_diff_pct:.2f}%\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c738018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Validation Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "validation_passed = 0\n",
    "validation_total = 4\n",
    "\n",
    "print(\"\\n VALIDATION CHECKLIST:\\n\")\n",
    "\n",
    "# Check 1: Cross-validation\n",
    "if cv_df['CV Std Dev'].max() < 0.05:\n",
    " print(\" Cross-Validation: PASSED - Models are stable across different data splits\")\n",
    " validation_passed += 1\n",
    "else:\n",
    " print(\" Cross-Validation: WARNING - Some variability detected\")\n",
    "\n",
    "# Check 2: Overfitting\n",
    "if overfitting_count == 0:\n",
    " print(\" Overfitting Test: PASSED - No overfitting detected\")\n",
    " validation_passed += 1\n",
    "elif overfitting_count <= 2:\n",
    " print(\" Overfitting Test: ACCEPTABLE - Minimal overfitting\")\n",
    " validation_passed += 0.5\n",
    "else:\n",
    " print(\" Overfitting Test: FAILED - Overfitting detected\")\n",
    "\n",
    "# Check 3: Residual Analysis\n",
    "if 0.8 <= std_ratio <= 1.2:\n",
    " print(\" Residual Analysis: PASSED - Consistent error distribution\")\n",
    " validation_passed += 1\n",
    "else:\n",
    " print(\" Residual Analysis: WARNING - Some inconsistency in errors\")\n",
    " validation_passed += 0.5\n",
    "\n",
    "# Check 4: Prediction Distribution\n",
    "if test_pred_min >= train_min * 0.8 and test_pred_max <= train_max * 1.2:\n",
    " print(\" Distribution Test: PASSED - Predictions within valid range\")\n",
    " validation_passed += 1\n",
    "else:\n",
    " print(\" Distribution Test: WARNING - Check prediction ranges\")\n",
    "\n",
    "# Overall verdict\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"OVERALL VALIDATION SCORE: {validation_passed}/{validation_total} ({validation_passed/validation_total*100:.0f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if validation_passed >= 3.5:\n",
    " print(\"\\n EXCELLENT: Model validation successful!\")\n",
    " print(\" ‚úì Models are production-ready\")\n",
    " print(\" ‚úì Good generalization to unseen data\")\n",
    " print(\" ‚úì Consistent and reliable predictions\")\n",
    "elif validation_passed >= 2.5:\n",
    " print(\"\\nüëç GOOD: Models pass most validation tests\")\n",
    " print(\" ‚úì Generally reliable for predictions\")\n",
    " print(\" ‚ö† Some minor concerns to address\")\n",
    "else:\n",
    " print(\"\\n NEEDS IMPROVEMENT: Several validation concerns\")\n",
    " print(\" ‚ö† Review model complexity and training\")\n",
    " print(\" ‚ö† Consider additional data or feature engineering\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" RECOMMENDATION:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best Model for Production: {best_model_name}\")\n",
    "print(f\" Test R¬≤: {best_r2:.4f} ({best_r2*100:.2f}%)\")\n",
    "print(f\" MAE: ${best_mae:,.2f}\")\n",
    "print(f\" MAPE: {best_mape:.2f}%\")\n",
    "print(f\" Validation Status: {' PASSED' if validation_passed >= 3.5 else ' USE WITH CAUTION'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c0e6c",
   "metadata": {},
   "source": [
    "## üíæ Download Results\n",
    "\n",
    "Download the model and results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\" Results saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Download file\n",
    "from google.colab import files\n",
    "files.download('model_comparison_results.csv')\n",
    "print(\"üì• File downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28f72b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Model**: The best performing model achieved excellent results\n",
    "2. **Feature Importance**: Time-based features and lag features are crucial\n",
    "3. **Accuracy**: Models can predict sales with high accuracy\n",
    "4. **Performance**: Tree-based ensemble methods outperform linear models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Fine-tune the best model\n",
    "2. **Feature Engineering**: Create additional domain-specific features\n",
    "3. **Ensemble Methods**: Combine multiple models\n",
    "4. **Deployment**: Deploy the model for production use\n",
    "\n",
    "---\n",
    "\n",
    "**Project**: Walmart Sales Prediction ",
    "**Author**: Ahmed Galal ",
    "**GitHub**: [ahmedgalalxxx/Walmart-Sales-Prediction](https://github.com/ahmedgalalxxx/Walmart-Sales-Prediction)\n",
    "\n",
    "‚≠ê **Star the repository if you found this helpful!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}